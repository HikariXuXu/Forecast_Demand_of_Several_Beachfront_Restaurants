{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\n\nPATH_CSV = '/kaggle/input/applications-of-deep-learning-wustlfall-2022/beach_demand_forecast/'\n\ndf_sales_train = pd.read_csv(os.path.join(PATH_CSV,\"sales_train.csv\"))\ndf_items = pd.read_csv(os.path.join(PATH_CSV,\"items.csv\"))\ndf_resturant = pd.read_csv(os.path.join(PATH_CSV,\"resturants.csv\"))\ndf_sales_test = pd.read_csv(os.path.join(PATH_CSV,\"sales_test.csv\"))\n\ndf_sales_train.date = pd.to_datetime(df_sales_train.date, errors='coerce') \ndf_sales_test.date = pd.to_datetime(df_sales_test.date, errors='coerce') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-29T16:19:33.327365Z","iopub.execute_input":"2022-11-29T16:19:33.327822Z","iopub.status.idle":"2022-11-29T16:19:33.535966Z","shell.execute_reply.started":"2022-11-29T16:19:33.327736Z","shell.execute_reply":"2022-11-29T16:19:33.534998Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"We begin by creating one long sequence that combines training and test data. The test data occurs just after the training \"in time\".","metadata":{}},{"cell_type":"code","source":"df_sales = pd.concat([df_sales_train,df_sales_test])\ndf_sales.columns = ['date','item_id','price','item_count','submit_id']\ndf_sales.loc[~df_sales.submit_id.isna(),'submit_id'] = df_sales[~df_sales.submit_id.isna()].submit_id.astype(int)\n\ndf_sales","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:19:33.537678Z","iopub.execute_input":"2022-11-29T16:19:33.538452Z","iopub.status.idle":"2022-11-29T16:19:33.574403Z","shell.execute_reply.started":"2022-11-29T16:19:33.538417Z","shell.execute_reply":"2022-11-29T16:19:33.572798Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"           date  item_id  price  item_count  submit_id\n0    2019-01-01        3  29.22         2.0        NaN\n1    2019-01-01        4  26.42        22.0        NaN\n2    2019-01-01       12   4.87         7.0        NaN\n3    2019-01-01       13   4.18        12.0        NaN\n4    2019-01-01       16   3.21       136.0        NaN\n...         ...      ...    ...         ...        ...\n9195 2021-12-31       96  21.93         NaN     9195.0\n9196 2021-12-31       97  28.65         NaN     9196.0\n9197 2021-12-31       98   5.00         NaN     9197.0\n9198 2021-12-31       99   5.32         NaN     9198.0\n9199 2021-12-31      100   2.48         NaN     9199.0\n\n[109600 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>item_id</th>\n      <th>price</th>\n      <th>item_count</th>\n      <th>submit_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-01-01</td>\n      <td>3</td>\n      <td>29.22</td>\n      <td>2.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-01-01</td>\n      <td>4</td>\n      <td>26.42</td>\n      <td>22.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-01-01</td>\n      <td>12</td>\n      <td>4.87</td>\n      <td>7.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-01-01</td>\n      <td>13</td>\n      <td>4.18</td>\n      <td>12.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-01-01</td>\n      <td>16</td>\n      <td>3.21</td>\n      <td>136.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9195</th>\n      <td>2021-12-31</td>\n      <td>96</td>\n      <td>21.93</td>\n      <td>NaN</td>\n      <td>9195.0</td>\n    </tr>\n    <tr>\n      <th>9196</th>\n      <td>2021-12-31</td>\n      <td>97</td>\n      <td>28.65</td>\n      <td>NaN</td>\n      <td>9196.0</td>\n    </tr>\n    <tr>\n      <th>9197</th>\n      <td>2021-12-31</td>\n      <td>98</td>\n      <td>5.00</td>\n      <td>NaN</td>\n      <td>9197.0</td>\n    </tr>\n    <tr>\n      <th>9198</th>\n      <td>2021-12-31</td>\n      <td>99</td>\n      <td>5.32</td>\n      <td>NaN</td>\n      <td>9198.0</td>\n    </tr>\n    <tr>\n      <th>9199</th>\n      <td>2021-12-31</td>\n      <td>100</td>\n      <td>2.48</td>\n      <td>NaN</td>\n      <td>9199.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>109600 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Extract Image Data","metadata":{}},{"cell_type":"code","source":"import sys\n\n!git clone https://github.com/ultralytics/yolov5 --tag 6.2  # clone\n!mv /kaggle/working/6.2 /kaggle/working/yolov5\n%pip install -qr /kaggle/working/yolov5/requirements.txt  # install\nsys.path.insert(0,'/kaggle/working/yolov5/')\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:19:33.575570Z","iopub.execute_input":"2022-11-29T16:19:33.575886Z","iopub.status.idle":"2022-11-29T16:19:55.763687Z","shell.execute_reply.started":"2022-11-29T16:19:33.575861Z","shell.execute_reply":"2022-11-29T16:19:55.762531Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"YOLOv5 ðŸš€ v7.0-10-g10c025d Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","output_type":"stream"},{"name":"stdout","text":"Setup complete âœ… (2 CPUs, 15.6 GB RAM, 3964.5/4030.6 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"from os import walk\nimport datetime\nimport tqdm\nimport torch\n\n# Model\nyolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom\n\nPATH_CAM = '/kaggle/input/applications-of-deep-learning-wustlfall-2022/beach_demand_forecast/cam/'\nfilenames = next(walk(PATH_CAM), (None, None, []))[2]  \n\nlist_date = []\nlist_people = []\n\nfor file in tqdm.tqdm(filenames):\n    if file=='1.jpg': continue\n    filename = os.path.join(PATH_CAM, file)\n    results = yolo_model(filename)\n    df = results.pandas().xyxy[0]\n    people = len(df[df.name=='person'])\n    dt = datetime.datetime.strptime(file[:10], '%Y_%m_%d')\n    list_date.append(dt)\n    list_people.append(people)\n\ndf_street_view = pd.DataFrame({'date':list_date,'people':list_people})\ndf_street_view","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:19:55.766918Z","iopub.execute_input":"2022-11-29T16:19:55.767648Z","iopub.status.idle":"2022-11-29T16:20:44.898877Z","shell.execute_reply.started":"2022-11-29T16:19:55.767597Z","shell.execute_reply":"2022-11-29T16:20:44.897782Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\nYOLOv5 ðŸš€ v7.0-10-g10c025d Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/14.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ee4d3d0b684478943dd652d8c37f0c"}},"metadata":{}},{"name":"stderr","text":"\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\nAdding AutoShape... \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1097/1097 [00:31<00:00, 34.64it/s]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           date  people\n0    2020-01-04      14\n1    2019-07-15      18\n2    2021-09-13      18\n3    2021-06-15      20\n4    2019-05-31      18\n...         ...     ...\n1091 2020-08-29      21\n1092 2020-08-06      16\n1093 2019-03-07      16\n1094 2019-09-18      19\n1095 2020-10-22      15\n\n[1096 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>people</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-01-04</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-07-15</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-09-13</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-06-15</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-05-31</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1091</th>\n      <td>2020-08-29</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1092</th>\n      <td>2020-08-06</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1093</th>\n      <td>2019-03-07</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1094</th>\n      <td>2019-09-18</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1095</th>\n      <td>2020-10-22</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1096 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Utility function to create sequences.","metadata":{}},{"cell_type":"code","source":"def process_title(model, name):\n    v = None\n    i = 0\n    for word in name.split(' '):\n        word = word.lower()\n    if word == 'vegi': word = \"vegetable\"\n    if word == 'smoothy': word = \"malt\"\n    i+=1\n    if v is None and word in model:\n        v=model[word].copy()\n    elif word in model:\n        v+=model[word]\n    v/=i\n    return v\n\ndef series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n    \ndef drop_column(df, col):\n    columns_to_drop = [('%s(t+%d)' % (col, lag_size))]\n    for i in range(window, 0, -1):\n        columns_to_drop += [('%s(t-%d)' % (col, i))]\n    df.drop(columns_to_drop, axis=1, inplace=True, errors='ignore')\n    df.drop([f\"{col}(t)\"], axis=1, inplace=True, errors='ignore')\n\ndef cat_seq(df, col):\n    return to_categorical(df[col].values)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:20:44.901876Z","iopub.execute_input":"2022-11-29T16:20:44.902160Z","iopub.status.idle":"2022-11-29T16:20:44.916343Z","shell.execute_reply.started":"2022-11-29T16:20:44.902134Z","shell.execute_reply":"2022-11-29T16:20:44.915436Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Load the Glove Embeddings","metadata":{}},{"cell_type":"code","source":"!wget -c \"https://nlp.stanford.edu/data/glove.6B.zip\"\n!unzip glove.6B.zip\n\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_file = 'glove.6B.300d.txt'\ntmp_file = get_tmpfile(\"test_word2vec.txt\")\n_ = glove2word2vec(glove_file, tmp_file)\nw2vec_model = KeyedVectors.load_word2vec_format(tmp_file)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:20:44.917876Z","iopub.execute_input":"2022-11-29T16:20:44.918235Z","iopub.status.idle":"2022-11-29T16:26:42.680432Z","shell.execute_reply.started":"2022-11-29T16:20:44.918201Z","shell.execute_reply":"2022-11-29T16:26:42.679348Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2022-11-29 16:20:45--  https://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2022-11-29 16:20:46--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: â€˜glove.6B.zipâ€™\n\nglove.6B.zip        100%[===================>] 822.24M  5.12MB/s    in 2m 40s  \n\n2022-11-29 16:23:27 (5.13 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n\nArchive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n  # Remove the CWD from sys.path while we load stuff.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Engineer Time Series Features","metadata":{}},{"cell_type":"code","source":"item_lookup = {}\nfor i, name in zip(list(df_items.id),list(df_items.name)):\n    v = process_title(w2vec_model,name)\n    item_lookup[i] = v\n\n# Join the items and sales tables so that we can look up the store id and kcal for each item.\ndf_items2 = df_items[['id', 'store_id', 'kcal']]\ndf_train = df_sales.merge(df_items2,left_on='item_id',right_on='id')\ndf_train[['date','item_id','item_count','store_id']]\n\n# Merge people counts (new)\ntemp = len(df_train)\ndf_train = df_train.merge(df_street_view)\nassert len(df_train) == temp\n\n# Sort/agg\ndf_train = df_train.sort_values('date').groupby(['item_id', 'store_id', 'date'], as_index=False)\ndf_train = df_train.agg({'item_count':['mean'], 'people':['mean'], 'price':['mean'], 'kcal':['mean'],'submit_id':['mean']})\ndf_train.columns = ['item', 'store', 'date', 'sales', 'people', 'price', 'kcal', 'submit_id']\ndf_train['dow'] = df_train['date'].dt.dayofweek\ndf_train['doy'] = df_train['date'].dt.dayofyear\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:42.682093Z","iopub.execute_input":"2022-11-29T16:26:42.683172Z","iopub.status.idle":"2022-11-29T16:26:42.869652Z","shell.execute_reply.started":"2022-11-29T16:26:42.683130Z","shell.execute_reply":"2022-11-29T16:26:42.868576Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   item  store       date  sales  people  price   kcal  submit_id  dow  doy\n0     1      4 2019-01-01    0.0    17.0   6.71  554.0        NaN    1    1\n1     1      4 2019-01-02    0.0    16.0   6.71  554.0        NaN    2    2\n2     1      4 2019-01-03    0.0    18.0   6.71  554.0        NaN    3    3\n3     1      4 2019-01-04    0.0    16.0   6.71  554.0        NaN    4    4\n4     1      4 2019-01-05    0.0    18.0   6.71  554.0        NaN    5    5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>store</th>\n      <th>date</th>\n      <th>sales</th>\n      <th>people</th>\n      <th>price</th>\n      <th>kcal</th>\n      <th>submit_id</th>\n      <th>dow</th>\n      <th>doy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2019-01-01</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2019-01-02</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2019-01-03</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2019-01-04</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2019-01-05</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Join the items and sales tables so that we can look up the store id for each item.","metadata":{}},{"cell_type":"markdown","source":"Determine the time gap between the last day from training set from the last day of the test set, this will be out lag (the amount of day that need to be forecast).","metadata":{}},{"cell_type":"code","source":"lag_size = (df_sales_test['date'].max().date() - df_sales_train['date'].max().date()).days\nprint('Max date from train set: %s' % df_sales_train['date'].max().date())\nprint('Max date from test set: %s' % df_sales_test['date'].max().date())\nprint('Forecast lag size', lag_size)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:42.870929Z","iopub.execute_input":"2022-11-29T16:26:42.871607Z","iopub.status.idle":"2022-11-29T16:26:42.882107Z","shell.execute_reply.started":"2022-11-29T16:26:42.871570Z","shell.execute_reply":"2022-11-29T16:26:42.880937Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Max date from train set: 2021-09-30\nMax date from test set: 2021-12-31\nForecast lag size 92\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Build the sequence data.","metadata":{}},{"cell_type":"code","source":"window = 30\nseries = series_to_supervised(df_train.drop('date', axis=1), window=window, lag=lag_size, dropnan=False)\nseries.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:42.886566Z","iopub.execute_input":"2022-11-29T16:26:42.886864Z","iopub.status.idle":"2022-11-29T16:26:43.306704Z","shell.execute_reply.started":"2022-11-29T16:26:42.886839Z","shell.execute_reply":"2022-11-29T16:26:43.305591Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   item(t-30)  store(t-30)  sales(t-30)  people(t-30)  price(t-30)  ...  \\\n0         NaN          NaN          NaN           NaN          NaN  ...   \n1         NaN          NaN          NaN           NaN          NaN  ...   \n2         NaN          NaN          NaN           NaN          NaN  ...   \n3         NaN          NaN          NaN           NaN          NaN  ...   \n4         NaN          NaN          NaN           NaN          NaN  ...   \n\n   price(t+92)  kcal(t+92)  submit_id(t+92)  dow(t+92)  doy(t+92)  \n0         6.71       554.0              NaN        2.0       93.0  \n1         6.71       554.0              NaN        3.0       94.0  \n2         6.71       554.0              NaN        4.0       95.0  \n3         6.71       554.0              NaN        5.0       96.0  \n4         6.71       554.0              NaN        6.0       97.0  \n\n[5 rows x 288 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item(t-30)</th>\n      <th>store(t-30)</th>\n      <th>sales(t-30)</th>\n      <th>people(t-30)</th>\n      <th>price(t-30)</th>\n      <th>...</th>\n      <th>price(t+92)</th>\n      <th>kcal(t+92)</th>\n      <th>submit_id(t+92)</th>\n      <th>dow(t+92)</th>\n      <th>doy(t+92)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>93.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>94.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>95.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>96.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>6.71</td>\n      <td>554.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>97.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 288 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Remove sequences that did not have enough data.","metadata":{}},{"cell_type":"code","source":"# Remove edge cases, where there were not enough values to complete a series\nlast_item = 'item(t-%d)' % window\nlast_store = 'store(t-%d)' % window\nseries = series[(series['store(t)'] == series[last_store])]\nseries = series[(series['item(t)'] == series[last_item])]\nseries = series[(series['store(t+%d)' % lag_size] == series[last_store])]\nseries = series[(series['item(t+%d)' % lag_size] == series[last_item])]","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:43.308084Z","iopub.execute_input":"2022-11-29T16:26:43.308861Z","iopub.status.idle":"2022-11-29T16:26:44.662549Z","shell.execute_reply.started":"2022-11-29T16:26:43.308809Z","shell.execute_reply":"2022-11-29T16:26:44.661474Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Split the training series and submit series","metadata":{}},{"cell_type":"code","source":"submit_id_col = 'submit_id(t+%d)' % lag_size\nlabels_col = 'sales(t+%d)' % lag_size\n\nseries_train = series.loc[series[submit_id_col].isna()].copy(deep=True)\nseries_submit = series.loc[~series[submit_id_col].isna()].copy(deep=True)\nseries_submit = series_submit.drop(labels_col, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:44.664004Z","iopub.execute_input":"2022-11-29T16:26:44.665146Z","iopub.status.idle":"2022-11-29T16:26:44.949717Z","shell.execute_reply.started":"2022-11-29T16:26:44.665071Z","shell.execute_reply":"2022-11-29T16:26:44.948673Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Drop all columns except sales\ndrop_column(series_train,'submit_id')\nseries_train.dropna(inplace=True)\n#series_submit.dropna(inplace=True)\nsubmit_id = series_submit[submit_id_col].astype(int)\ndrop_column(series_submit,'submit_id')\n#series_submit.drop(labels_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:44.951012Z","iopub.execute_input":"2022-11-29T16:26:44.951368Z","iopub.status.idle":"2022-11-29T16:26:45.200150Z","shell.execute_reply.started":"2022-11-29T16:26:44.951333Z","shell.execute_reply":"2022-11-29T16:26:45.199130Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Extract the predictors (x sequences) and the label (future prediction)","metadata":{}},{"cell_type":"markdown","source":"##### Training set","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.utils.np_utils import to_categorical   \n\n# Label\nlabels = series_train[labels_col]\n\n# item information and predicting information\nseries1_train = series_train[['item(t+%d)' % lag_size, 'store(t+%d)' % lag_size, 'dow(t+%d)' % lag_size, 'doy(t+%d)' % lag_size, 'people(t+%d)' % lag_size, 'price(t+%d)' % lag_size, 'kcal(t+%d)' % lag_size]]\n\nseries_train.drop(labels_col, axis=1, inplace=True)\nseries_train.drop('item(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('store(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('dow(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('doy(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('people(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('price(t+%d)' % lag_size, axis=1, inplace=True)\nseries_train.drop('kcal(t+%d)' % lag_size, axis=1, inplace=True)\n\n# Get sales sequences\nseries2_train = series_train.copy()\ndrop_column(series2_train, \"item\")\ndrop_column(series2_train, \"store\")\ndrop_column(series2_train, \"dow\")\ndrop_column(series2_train, \"doy\")\ndrop_column(series2_train, \"people\")\ndrop_column(series2_train, \"price\")\ndrop_column(series2_train, \"kcal\")\nsales_series_train = series2_train.values\n\n# Day of week as a number\nseries2_train = series_train.copy()\ndrop_column(series2_train, \"item\")\ndrop_column(series2_train, \"store\")\ndrop_column(series2_train, \"sales\")\ndrop_column(series2_train, \"doy\")\ndrop_column(series2_train, \"people\")\ndrop_column(series2_train, \"price\")\ndrop_column(series2_train, \"kcal\")\ndow_series_train = series2_train.values\n\n# Get day of year sequences\nseries2_train = series_train.copy()\ndrop_column(series2_train, \"item\")\ndrop_column(series2_train, \"store\")\ndrop_column(series2_train, \"dow\")\ndrop_column(series2_train, \"sales\")\ndrop_column(series2_train, \"people\")\ndrop_column(series2_train, \"price\")\ndrop_column(series2_train, \"kcal\")\ndoy_series_train = series2_train.values\n\n# Get number of people sequences\nseries2_train = series_train.copy()\ndrop_column(series2_train, \"item\")\ndrop_column(series2_train, \"store\")\ndrop_column(series2_train, \"dow\")\ndrop_column(series2_train, \"doy\")\ndrop_column(series2_train, \"sales\")\ndrop_column(series2_train, \"price\")\ndrop_column(series2_train, \"kcal\")\npeople_series_train = series2_train.values\n\n\n# Create x\nt1_train = sales_series_train.reshape(sales_series_train.shape + (1,))\nt2_train = dow_series_train.reshape(dow_series_train.shape + (1,)) \nt3_train = doy_series_train.reshape(doy_series_train.shape + (1,))\nt4_train = people_series_train.reshape(people_series_train.shape + (1,))\nx1_train = np.concatenate([t1_train,t2_train,t3_train,t4_train],axis=2)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:45.201934Z","iopub.execute_input":"2022-11-29T16:26:45.202310Z","iopub.status.idle":"2022-11-29T16:26:52.712740Z","shell.execute_reply.started":"2022-11-29T16:26:45.202274Z","shell.execute_reply":"2022-11-29T16:26:52.711682Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(t1_train.shape)\nprint(t2_train.shape)\nprint(t3_train.shape)\nprint(t4_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.714283Z","iopub.execute_input":"2022-11-29T16:26:52.714670Z","iopub.status.idle":"2022-11-29T16:26:52.721373Z","shell.execute_reply.started":"2022-11-29T16:26:52.714612Z","shell.execute_reply":"2022-11-29T16:26:52.719354Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(88200, 31, 1)\n(88200, 31, 1)\n(88200, 31, 1)\n(88200, 31, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create predictors (x)\nvec_size = w2vec_model['test'].shape[0]\n\nlst = []\nfor item in list(series_train['item(t-1)']):\n    lst.append(item_lookup[item])\n\nx2_train = np.concatenate(lst).reshape((series_train.shape[0],vec_size))\n\nx3_train = series1_train.values\n\n#x_train = [x1_train,x2_train,x3_train]\nx_train = [x1_train,x3_train]","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.723288Z","iopub.execute_input":"2022-11-29T16:26:52.724020Z","iopub.status.idle":"2022-11-29T16:26:52.804322Z","shell.execute_reply.started":"2022-11-29T16:26:52.723984Z","shell.execute_reply":"2022-11-29T16:26:52.803323Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"##### Submit set","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.utils.np_utils import to_categorical   \n\n\n# item information and predicting information\nseries1_submit = series_submit[['item(t+%d)' % lag_size, 'store(t+%d)' % lag_size, 'dow(t+%d)' % lag_size, 'doy(t+%d)' % lag_size, 'people(t+%d)' % lag_size, 'price(t+%d)' % lag_size, 'kcal(t+%d)' % lag_size]]\n\nseries_submit.drop('item(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('store(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('dow(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('doy(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('people(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('price(t+%d)' % lag_size, axis=1, inplace=True)\nseries_submit.drop('kcal(t+%d)' % lag_size, axis=1, inplace=True)\n\n# Get sales sequences\nseries2_submit = series_submit.copy()\ndrop_column(series2_submit, \"item\")\ndrop_column(series2_submit, \"store\")\ndrop_column(series2_submit, \"dow\")\ndrop_column(series2_submit, \"doy\")\ndrop_column(series2_submit, \"people\")\ndrop_column(series2_submit, \"price\")\ndrop_column(series2_submit, \"kcal\")\nsales_series_submit = series2_submit.values\n\n# Day of week as a number\nseries2_submit = series_submit.copy()\ndrop_column(series2_submit, \"item\")\ndrop_column(series2_submit, \"store\")\ndrop_column(series2_submit, \"sales\")\ndrop_column(series2_submit, \"doy\")\ndrop_column(series2_submit, \"people\")\ndrop_column(series2_submit, \"price\")\ndrop_column(series2_submit, \"kcal\")\ndow_series_submit = series2_submit.values\n\n# Get day of year sequences\nseries2_submit = series_submit.copy()\ndrop_column(series2_submit, \"item\")\ndrop_column(series2_submit, \"store\")\ndrop_column(series2_submit, \"dow\")\ndrop_column(series2_submit, \"sales\")\ndrop_column(series2_submit, \"people\")\ndrop_column(series2_submit, \"price\")\ndrop_column(series2_submit, \"kcal\")\ndoy_series_submit = series2_submit.values\n\n# Get number of people sequences\nseries2_submit = series_submit.copy()\ndrop_column(series2_submit, \"item\")\ndrop_column(series2_submit, \"store\")\ndrop_column(series2_submit, \"dow\")\ndrop_column(series2_submit, \"doy\")\ndrop_column(series2_submit, \"sales\")\ndrop_column(series2_submit, \"price\")\ndrop_column(series2_submit, \"kcal\")\npeople_series_submit = series2_submit.values\n\n\n# Create x\nt1_submit = sales_series_submit.reshape(sales_series_submit.shape + (1,))\nt2_submit = dow_series_submit.reshape(dow_series_submit.shape + (1,)) \nt3_submit = doy_series_submit.reshape(doy_series_submit.shape + (1,))\nt4_submit = people_series_submit.reshape(people_series_submit.shape + (1,))\nx1_submit = np.concatenate([t1_submit,t2_submit,t3_submit,t4_submit],axis=2)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.805948Z","iopub.execute_input":"2022-11-29T16:26:52.806312Z","iopub.status.idle":"2022-11-29T16:26:52.954877Z","shell.execute_reply.started":"2022-11-29T16:26:52.806272Z","shell.execute_reply":"2022-11-29T16:26:52.953885Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(t1_submit.shape)\nprint(t2_submit.shape)\nprint(t3_submit.shape)\nprint(t4_submit.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.956467Z","iopub.execute_input":"2022-11-29T16:26:52.956870Z","iopub.status.idle":"2022-11-29T16:26:52.963749Z","shell.execute_reply.started":"2022-11-29T16:26:52.956833Z","shell.execute_reply":"2022-11-29T16:26:52.962760Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(9200, 31, 1)\n(9200, 31, 1)\n(9200, 31, 1)\n(9200, 31, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create predictors (x)\nvec_size = w2vec_model['test'].shape[0]\n\nlst = []\nfor item in list(series_submit['item(t-1)']):\n    lst.append(item_lookup[item])\n\nx2_submit = np.concatenate(lst).reshape((series_submit.shape[0],vec_size))\n\nx3_submit = series1_submit.values\n\n#x_submit = [x1_submit,x2_submit,x3_submit]\nx_submit = [x1_submit,x3_submit]","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.965109Z","iopub.execute_input":"2022-11-29T16:26:52.966151Z","iopub.status.idle":"2022-11-29T16:26:52.981058Z","shell.execute_reply.started":"2022-11-29T16:26:52.966117Z","shell.execute_reply":"2022-11-29T16:26:52.980215Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Train the Network\nExtract the predictors (x sequences) and the label (future prediction)","metadata":{}},{"cell_type":"code","source":"TEST_SIZE = 0.25\n\nmask = np.random.random(size=x_train[0].shape[0])<TEST_SIZE\n\nX_train = []\nX_valid = []\n\nfor subx in x_train:\n    X_train.append(subx[~mask])\n    X_valid.append(subx[mask])\n\nY_train = labels.values[~mask]\nY_valid = labels.values[mask]\n\nprint('Train set shape x1:', X_train[0].shape)\nprint('Train set shape x2:', X_train[1].shape)\nprint('Validation set shape x1:', X_valid[0].shape)\nprint('Validation set shape x2:', X_valid[1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:26:52.983929Z","iopub.execute_input":"2022-11-29T16:26:52.984206Z","iopub.status.idle":"2022-11-29T16:26:53.078951Z","shell.execute_reply.started":"2022-11-29T16:26:52.984182Z","shell.execute_reply":"2022-11-29T16:26:53.077692Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Train set shape x1: (66036, 31, 4)\nTrain set shape x2: (66036, 7)\nValidation set shape x1: (22164, 31, 4)\nValidation set shape x2: (22164, 7)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Construct the neural network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf \nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, concatenate, Input\nimport keras\n\nepochs = 500\nbatch = 256\nlr = 0.0003\nadam = tf.keras.optimizers.Adam(lr)\nsgd = tf.keras.optimizers.SGD(learning_rate=lr)\n\nmodel = Sequential()\n\nA1 = Input(shape=(X_train[0].shape[1], X_train[0].shape[2]),name='A1')\nA2 = Conv1D(filters=64, kernel_size=8, activation='relu')(A1)\nA3 = MaxPooling1D(pool_size=2)(A2)\nA4 = Conv1D(filters=32, kernel_size=8, activation='relu')(A3)\nA5 = MaxPooling1D(pool_size=2)(A4)\nA6 = Flatten()(A5)\nA7 = Dense(64, activation='relu')(A6)\nA8 = Dropout(0.4)(A7)\nA9 = Dense(32, activation='relu')(A8)\nA10 = Dropout(0.4)(A9)\n\n#B1 = Input(shape=X_train[1].shape[1],name='B1')\n#B2 = Dense(16, activation='relu',name='B2')(B1)\n#B3 = Dropout(0.2)(B2)\n#B4 = Dense(8, activation='relu',name='B4')(B3)\n\nC1 = Input(shape=X_train[1].shape[1], name='C1')\nC2 = Dense(16, activation='relu', name='C2')(C1)\n#C3 = Dropout(0.2)(C2)\n#C4 = Dense(8, activation='relu',name='C4')(C3)\n\n#M1 = concatenate([A8,B4,C4])\nM1 = concatenate([A10,C2])\nM2 = Dense(16,name='M2')(M1)\nM3 = Dropout(0.2)(M2)\nM4 = Dense(1,name='M4')(M3)\n\n#model = Model(inputs=[A1, B1, C1],outputs=[M4])\nmodel = Model(inputs=[A1, C1],outputs=[M4])\nmodel.compile(loss='mse', optimizer=adam)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-29T17:06:41.246568Z","iopub.execute_input":"2022-11-29T17:06:41.246936Z","iopub.status.idle":"2022-11-29T17:06:41.332701Z","shell.execute_reply.started":"2022-11-29T17:06:41.246906Z","shell.execute_reply":"2022-11-29T17:06:41.331610Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Model: \"model_8\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nA1 (InputLayer)                 [(None, 31, 4)]      0                                            \n__________________________________________________________________________________________________\nconv1d_10 (Conv1D)              (None, 24, 64)       2112        A1[0][0]                         \n__________________________________________________________________________________________________\nmax_pooling1d_10 (MaxPooling1D) (None, 12, 64)       0           conv1d_10[0][0]                  \n__________________________________________________________________________________________________\nconv1d_11 (Conv1D)              (None, 5, 32)        16416       max_pooling1d_10[0][0]           \n__________________________________________________________________________________________________\nmax_pooling1d_11 (MaxPooling1D) (None, 2, 32)        0           conv1d_11[0][0]                  \n__________________________________________________________________________________________________\nflatten_9 (Flatten)             (None, 64)           0           max_pooling1d_11[0][0]           \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 64)           4160        flatten_9[0][0]                  \n__________________________________________________________________________________________________\ndropout_26 (Dropout)            (None, 64)           0           dense_18[0][0]                   \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 32)           2080        dropout_26[0][0]                 \n__________________________________________________________________________________________________\nC1 (InputLayer)                 [(None, 7)]          0                                            \n__________________________________________________________________________________________________\ndropout_27 (Dropout)            (None, 32)           0           dense_19[0][0]                   \n__________________________________________________________________________________________________\nC2 (Dense)                      (None, 16)           128         C1[0][0]                         \n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 48)           0           dropout_27[0][0]                 \n                                                                 C2[0][0]                         \n__________________________________________________________________________________________________\nM2 (Dense)                      (None, 16)           784         concatenate_8[0][0]              \n__________________________________________________________________________________________________\ndropout_28 (Dropout)            (None, 16)           0           M2[0][0]                         \n__________________________________________________________________________________________________\nM4 (Dense)                      (None, 1)            17          dropout_28[0][0]                 \n==================================================================================================\nTotal params: 25,697\nTrainable params: 25,697\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Fit the neural network.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, \n        verbose=1, mode='auto', restore_best_weights=True)\n\ncnn_history = model.fit(X_train, Y_train, callbacks=[monitor],\n    validation_data=(X_valid, Y_valid), epochs=epochs, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T17:08:49.376708Z","iopub.execute_input":"2022-11-29T17:08:49.377252Z","iopub.status.idle":"2022-11-29T17:12:26.151237Z","shell.execute_reply.started":"2022-11-29T17:08:49.377212Z","shell.execute_reply":"2022-11-29T17:12:26.150302Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1/500\n2064/2064 - 7s - loss: 205.7678 - val_loss: 195.8160\nEpoch 2/500\n2064/2064 - 7s - loss: 194.8998 - val_loss: 111.4221\nEpoch 3/500\n2064/2064 - 6s - loss: 199.4159 - val_loss: 114.7372\nEpoch 4/500\n2064/2064 - 7s - loss: 193.4206 - val_loss: 125.6539\nEpoch 5/500\n2064/2064 - 7s - loss: 190.2381 - val_loss: 107.3292\nEpoch 6/500\n2064/2064 - 7s - loss: 194.0457 - val_loss: 194.6439\nEpoch 7/500\n2064/2064 - 7s - loss: 195.5252 - val_loss: 109.6966\nEpoch 8/500\n2064/2064 - 6s - loss: 187.1138 - val_loss: 102.1987\nEpoch 9/500\n2064/2064 - 7s - loss: 195.7028 - val_loss: 106.3573\nEpoch 10/500\n2064/2064 - 7s - loss: 204.4639 - val_loss: 101.2723\nEpoch 11/500\n2064/2064 - 6s - loss: 192.9635 - val_loss: 106.1486\nEpoch 12/500\n2064/2064 - 7s - loss: 184.1046 - val_loss: 102.8355\nEpoch 13/500\n2064/2064 - 6s - loss: 192.8688 - val_loss: 108.7577\nEpoch 14/500\n2064/2064 - 7s - loss: 180.4367 - val_loss: 177.4986\nEpoch 15/500\n2064/2064 - 7s - loss: 181.2008 - val_loss: 105.2366\nEpoch 16/500\n2064/2064 - 7s - loss: 176.8828 - val_loss: 120.6024\nEpoch 17/500\n2064/2064 - 7s - loss: 187.2680 - val_loss: 95.4168\nEpoch 18/500\n2064/2064 - 7s - loss: 177.7504 - val_loss: 95.8828\nEpoch 19/500\n2064/2064 - 7s - loss: 177.0767 - val_loss: 95.0412\nEpoch 20/500\n2064/2064 - 7s - loss: 174.0100 - val_loss: 93.5531\nEpoch 21/500\n2064/2064 - 7s - loss: 182.8430 - val_loss: 100.3560\nEpoch 22/500\n2064/2064 - 7s - loss: 170.1028 - val_loss: 92.4491\nEpoch 23/500\n2064/2064 - 7s - loss: 177.7301 - val_loss: 142.1999\nEpoch 24/500\n2064/2064 - 7s - loss: 184.7085 - val_loss: 104.1581\nEpoch 25/500\n2064/2064 - 7s - loss: 183.8012 - val_loss: 94.6340\nEpoch 26/500\n2064/2064 - 7s - loss: 181.2785 - val_loss: 107.7842\nEpoch 27/500\n2064/2064 - 7s - loss: 175.9858 - val_loss: 93.7931\nEpoch 28/500\n2064/2064 - 7s - loss: 175.9259 - val_loss: 106.3510\nEpoch 29/500\n2064/2064 - 6s - loss: 166.3167 - val_loss: 127.2300\nEpoch 30/500\n2064/2064 - 7s - loss: 166.7590 - val_loss: 96.7805\nEpoch 31/500\n2064/2064 - 7s - loss: 162.4438 - val_loss: 129.3203\nEpoch 32/500\n2064/2064 - 6s - loss: 172.4723 - val_loss: 110.9117\nRestoring model weights from the end of the best epoch.\nEpoch 00032: early stopping\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Predict and evaluate the validation data.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\ncnn_train_pred = model.predict(X_train)\ncnn_valid_pred = model.predict(X_valid)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-11-29T17:13:14.736838Z","iopub.execute_input":"2022-11-29T17:13:14.737212Z","iopub.status.idle":"2022-11-29T17:13:18.594360Z","shell.execute_reply.started":"2022-11-29T17:13:14.737180Z","shell.execute_reply":"2022-11-29T17:13:18.593317Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Train rmse: 10.759702571207375\nValidation rmse: 9.615046609777272\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Plot the training curve.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nplt.plot(cnn_history.history['loss'], label='Train loss')\nplt.plot(cnn_history.history['val_loss'], label='Validation loss')\nfig.legend()\nfig.suptitle('CNN')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MSE\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:58:32.867574Z","iopub.execute_input":"2022-11-29T16:58:32.868809Z","iopub.status.idle":"2022-11-29T16:58:33.010887Z","shell.execute_reply.started":"2022-11-29T16:58:32.868764Z","shell.execute_reply":"2022-11-29T16:58:33.009520Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Build a Submission File","metadata":{}},{"cell_type":"code","source":"submit_pred = model.predict(x_submit)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:58:34.928759Z","iopub.execute_input":"2022-11-29T16:58:34.929143Z","iopub.status.idle":"2022-11-29T16:58:35.603003Z","shell.execute_reply.started":"2022-11-29T16:58:34.929111Z","shell.execute_reply":"2022-11-29T16:58:35.601914Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.DataFrame()\ndf_submit['id'] = submit_id.to_list()\ndf_submit['item_count'] = submit_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:58:35.732902Z","iopub.execute_input":"2022-11-29T16:58:35.733886Z","iopub.status.idle":"2022-11-29T16:58:35.747566Z","shell.execute_reply.started":"2022-11-29T16:58:35.733841Z","shell.execute_reply":"2022-11-29T16:58:35.746569Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df_submit.to_csv('submit.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:58:36.690051Z","iopub.execute_input":"2022-11-29T16:58:36.690416Z","iopub.status.idle":"2022-11-29T16:58:36.713395Z","shell.execute_reply.started":"2022-11-29T16:58:36.690386Z","shell.execute_reply":"2022-11-29T16:58:36.712494Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Download the Submit File\n\nYou only need to do this if you wish to view it locally. Otherwise, submit through Kaggle.","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(r'submit.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-29T16:58:41.595597Z","iopub.execute_input":"2022-11-29T16:58:41.595984Z","iopub.status.idle":"2022-11-29T16:58:41.603572Z","shell.execute_reply.started":"2022-11-29T16:58:41.595952Z","shell.execute_reply":"2022-11-29T16:58:41.602482Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submit.csv","text/html":"<a href='submit.csv' target='_blank'>submit.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}